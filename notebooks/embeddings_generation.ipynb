{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet('../data/BindingDB_predprocessed/BindingDB_v0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataic50 = df[[\"Ligand SMILES\",\"IC50 (nM)\",\"BindingDB Target Chain Sequence\"]].drop_nulls() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_687_796, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Ligand SMILES</th><th>IC50 (nM)</th><th>BindingDB Target Chain Sequence</th></tr><tr><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;NS(=O)(=O)c1ccc(Nc2cc(OCC3CCCC…</td><td>29000.0</td><td>&quot;MSGRPRTTSFAESCKPVQQPSAFGSMKVSR…</td></tr><tr><td>&quot;NS(=O)(=O)c1ccc(Nc2cc(OC3CCCCC…</td><td>190.0</td><td>&quot;MSGRPRTTSFAESCKPVQQPSAFGSMKVSR…</td></tr><tr><td>&quot;NS(=O)(=O)c1ccc(Nc2cc(NC3CCCCC…</td><td>970.0</td><td>&quot;MSGRPRTTSFAESCKPVQQPSAFGSMKVSR…</td></tr><tr><td>&quot;CCN(CC)c1cc(Nc2ccc(cc2)S(N)(=O…</td><td>11000.0</td><td>&quot;MSGRPRTTSFAESCKPVQQPSAFGSMKVSR…</td></tr><tr><td>&quot;N[C@H]1CC[C@@H](CC1)Nc1cc(Nc2c…</td><td>780.0</td><td>&quot;MSGRPRTTSFAESCKPVQQPSAFGSMKVSR…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;O[C@@H]1CCCN(C1)C(=O)c1cccc(c1…</td><td>90.0</td><td>&quot;MSSWIRWHGPAMARLWGFCWLVVGFWRAAF…</td></tr><tr><td>&quot;O[C@H]1CCCN(C1)C(=O)c1cccc(c1)…</td><td>118.0</td><td>&quot;MSSWIRWHGPAMARLWGFCWLVVGFWRAAF…</td></tr><tr><td>&quot;COc1nc2ccc(Br)cc2cc1[C@@H](c1c…</td><td>1600.0</td><td>&quot;MPVRRGHVAPQNTFLDTIIRKFEGQSRKFI…</td></tr><tr><td>&quot;COc1ccc(cc1)N(C)c1nc(C)nc2[nH]…</td><td>2600.0</td><td>&quot;CVSASPSTLARLVSRSAMPAGSSTAWNTAF…</td></tr><tr><td>&quot;COc1ccc(\\C=C/c2cc(OC)c(OC)c(OC…</td><td>1000.0</td><td>&quot;CVSASPSTLARLVSRSAMPAGSSTAWNTAF…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_687_796, 3)\n",
       "┌─────────────────────────────────┬───────────┬─────────────────────────────────┐\n",
       "│ Ligand SMILES                   ┆ IC50 (nM) ┆ BindingDB Target Chain Sequenc… │\n",
       "│ ---                             ┆ ---       ┆ ---                             │\n",
       "│ str                             ┆ f64       ┆ str                             │\n",
       "╞═════════════════════════════════╪═══════════╪═════════════════════════════════╡\n",
       "│ NS(=O)(=O)c1ccc(Nc2cc(OCC3CCCC… ┆ 29000.0   ┆ MSGRPRTTSFAESCKPVQQPSAFGSMKVSR… │\n",
       "│ NS(=O)(=O)c1ccc(Nc2cc(OC3CCCCC… ┆ 190.0     ┆ MSGRPRTTSFAESCKPVQQPSAFGSMKVSR… │\n",
       "│ NS(=O)(=O)c1ccc(Nc2cc(NC3CCCCC… ┆ 970.0     ┆ MSGRPRTTSFAESCKPVQQPSAFGSMKVSR… │\n",
       "│ CCN(CC)c1cc(Nc2ccc(cc2)S(N)(=O… ┆ 11000.0   ┆ MSGRPRTTSFAESCKPVQQPSAFGSMKVSR… │\n",
       "│ N[C@H]1CC[C@@H](CC1)Nc1cc(Nc2c… ┆ 780.0     ┆ MSGRPRTTSFAESCKPVQQPSAFGSMKVSR… │\n",
       "│ …                               ┆ …         ┆ …                               │\n",
       "│ O[C@@H]1CCCN(C1)C(=O)c1cccc(c1… ┆ 90.0      ┆ MSSWIRWHGPAMARLWGFCWLVVGFWRAAF… │\n",
       "│ O[C@H]1CCCN(C1)C(=O)c1cccc(c1)… ┆ 118.0     ┆ MSSWIRWHGPAMARLWGFCWLVVGFWRAAF… │\n",
       "│ COc1nc2ccc(Br)cc2cc1[C@@H](c1c… ┆ 1600.0    ┆ MPVRRGHVAPQNTFLDTIIRKFEGQSRKFI… │\n",
       "│ COc1ccc(cc1)N(C)c1nc(C)nc2[nH]… ┆ 2600.0    ┆ CVSASPSTLARLVSRSAMPAGSSTAWNTAF… │\n",
       "│ COc1ccc(\\C=C/c2cc(OC)c(OC)c(OC… ┆ 1000.0    ┆ CVSASPSTLARLVSRSAMPAGSSTAWNTAF… │\n",
       "└─────────────────────────────────┴───────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataic50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_smiles = dataic50['Ligand SMILES'].to_list()\n",
    "target_chain_sequence = dataic50['BindingDB Target Chain Sequence'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and move model to GPU\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "checkpoint = 'unikei/bert-base-smiles'\n",
    "tokenizer_smiles = BertTokenizerFast.from_pretrained(checkpoint)\n",
    "model_smiles = BertModel.from_pretrained(checkpoint)\n",
    "model_smiles.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer_protein = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model_protein = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model_protein.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def generate_embeddings_bert_based_models(batch, tokenizer, model,device):\n",
    "    inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().tolist()\n",
    "    return embeddings\n",
    "\n",
    "def generate_embeddings(batch, tokenizer, model,device):\n",
    "    inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 1:-1, :].mean(dim=1).cpu().numpy().tolist() \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a Parquet writer schema\n",
    "schema = pa.schema([\n",
    "    ('ID', pa.string()),\n",
    "    ('encoding', pa.list_(pa.float64()))\n",
    "])\n",
    "\n",
    "# Universal function to write embeddings to Parquet\n",
    "def write_embeddings_to_parquet(data, batch_size, parquet_file, generate_embedding_func, tokenizer, model, device):\n",
    "    num_newlines = len(data)\n",
    "    total_batches = (num_newlines + batch_size - 1) // batch_size\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    for batch in tqdm(create_batches(data, batch_size), total=total_batches, desc=\"Processing batches\"):\n",
    "        embeddings = generate_embedding_func(batch,tokenizer, model, device)\n",
    "        df = pd.DataFrame({'ID': batch, 'encoding': embeddings})\n",
    "        \n",
    "        table = pa.Table.from_pandas(df, schema=schema)\n",
    "        \n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(parquet_file, schema)\n",
    "        \n",
    "        writer.write_table(table)\n",
    "        \n",
    "        # Clear CUDA cache to free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if writer:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "parquet_file_smiles = '../data/embeddings/ligand_embeddings.parquet'\n",
    "parquet_file_protein = '../data/embeddings/protein_embeddings.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write SMILES embeddings to Parquet\n",
    "write_embeddings_to_parquet(ligand_smiles, batch_size, parquet_file_smiles, generate_embeddings, tokenizer_smiles, model_smiles, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/210975 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing batches: 100%|██████████| 210975/210975 [4:07:26<00:00, 14.21it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Write protein sequence embeddings to Parquet\n",
    "write_embeddings_to_parquet(target_chain_sequence, batch_size, parquet_file_protein, generate_embeddings, tokenizer_protein, model_protein, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemdiv-screening-project-ubgft5hy-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
